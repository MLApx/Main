{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketName = 'signet-ring-cell-bryan'\n",
    "folderName = 'challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket = s3_client.list_objects(Bucket=bucketName, Prefix=folderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/2018_64982_1-3_2019-02-25 21_57_36-lv0-33516-59515-2003-2010.xml\n",
      "challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/2018_64982_1-3_2019-02-25 21_57_36-lv0-34589-61706-2030-2044.xml\n",
      "challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/2018_64982_1-3_2019-02-25 21_57_36-lv0-36515-58465-2013-2071.xml\n",
      "challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/2018_64982_1-3_2019-02-25 21_57_36-lv0-37528-60747-2016-2000.xml\n",
      "challenge1-signetringcell-dataset/sig-train-pos/sig-train-pos/2018_64982_1-3_2019-02-25 21_57_36-lv0-38368-62991-2040-2016.xml\n"
     ]
    }
   ],
   "source": [
    "# First five xml files) in our dataset\n",
    "for img_num in range(1,11):\n",
    "    if img_num%2==1:\n",
    "        print(bucket['Contents'][img_num]['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bucket['Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create json annotation file from xml file and move that json file to s3\n",
    "def xml_to_json(data, bucketName, folderName, fileName):\n",
    "    \"\"\"\n",
    "    data: content being read\n",
    "    bucketName: bucket being read from\n",
    "    folderName: folder being wrote to\n",
    "    file_name: xml file being read\n",
    "    \"\"\"\n",
    "    Bs_data = BeautifulSoup(data, \"xml\") \n",
    "    zz=Bs_data.find_all(\"size\")\n",
    "    #print(zz)\n",
    "    yyyy=str(zz)\n",
    "    size=yyyy.partition('\\n')[2]\n",
    "    #print(size)\n",
    "    wid=size.split(\"</width>\",1)[0] \n",
    "    wid=wid.split(\"<width>\",1)[1]\n",
    "    #print(wid)\n",
    "    hei=size.split(\"</height>\",1)[0] \n",
    "    hei=hei.split(\"<height>\",1)[1]\n",
    "    #print(hei)\n",
    "    depth=size.split(\"</depth>\",1)[0] \n",
    "    depth=depth.split(\"<depth>\",1)[1]\n",
    "    #print(depth)\n",
    "    b_unique = Bs_data.find_all('bndbox') \n",
    "    data1={}\n",
    "   # data1['files']=file_name\n",
    "# I think we nned to change it to jpeg, i don't know if that would fix it\n",
    "    data1['files']=fileName.split('/')[-1][:-4] + '.jpg'\n",
    "    data1['image_size']=[]\n",
    "    data1['image_size'].append({\n",
    "        'width' : wid,\n",
    "        'height' : hei,\n",
    "        'depth' : depth\n",
    "    })\n",
    "    data1['annotations']=[]\n",
    "    for i in range(len(b_unique)):\n",
    "        z=b_unique[i]\n",
    "        y=str(z)\n",
    "        yy=y.partition('\\n')[2]\n",
    "        xmin=yy.split(\"</xmin>\",1)[0]\n",
    "        xmin=xmin.split(\"<xmin>\",1)[1]\n",
    "        ymin=yy.split(\"</ymin>\",1)[0] \n",
    "        ymin=ymin.split(\"<ymin>\",1)[1]\n",
    "        xmax=yy.split(\"</xmax>\",1)[0] \n",
    "        xmax=xmax.split(\"<xmax>\",1)[1]\n",
    "        ymax=yy.split(\"</ymax>\",1)[0] \n",
    "        ymax=ymax.split(\"<ymax>\",1)[1]\n",
    "        left=int(xmin)\n",
    "        top=int(ymax)\n",
    "        height=str(int(ymax)-int(ymin))\n",
    "        width=str(int(xmax)-int(xmin))\n",
    "        data1['annotations'].append({\n",
    "            'class_id': 0,\n",
    "            'left': left,\n",
    "            'top' : top,\n",
    "            'width' : width,\n",
    "            'height' :height\n",
    "        })\n",
    "    data1['categories']=[]\n",
    "    data1['categories'].append({\n",
    "        'class_id' : 0,\n",
    "        'name' : \"signet ring cell\"\n",
    "    })\n",
    "    \n",
    "    json_fileName = fileName.split('/')[-1][:-4] + '.json'\n",
    "    with open(json_fileName, 'w') as outfile:\n",
    "        print(\"writing json file \" + json_fileName)\n",
    "        json.dump(data1, outfile) \n",
    "        \n",
    "    with open(json_fileName, 'rb') as f:\n",
    "        print('sending file to s3')\n",
    "        s3_client.upload_fileobj(f, bucketName, folderName + json_fileName)\n",
    "        print('file successfully sent')\n",
    "        \n",
    "    os.remove(json_fileName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_folderName = 'challenge1-signetringcell-dataset/src_detection_model/train_annotation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-33516-59515-2003-2010.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-34589-61706-2030-2044.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-36515-58465-2013-2071.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-37528-60747-2016-2000.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-38368-62991-2040-2016.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-39555-58583-2006-2007.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-39896-60663-2008-2000.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-47662-55703-2020-2046.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_64982_1-3_2019-02-25 21_57_36-lv0-50380-56565-2031-2015.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67247_1-4_2019-02-25 23_30_15-lv0-11882-64596-2042-2032.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67247_1-4_2019-02-25 23_30_15-lv0-14060-64077-2006-2011.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67247_1-4_2019-02-25 23_30_15-lv0-16770-59565-2026-2068.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67248_1-6_2019-02-25 23_52_19-lv0-56452-38197-2017-2032.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67248_1-6_2019-02-25 23_52_19-lv0-59344-38775-2064-2048.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67248_1-6_2019-02-25 23_52_19-lv0-67097-37769-2048-2033.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67248_1-6_2019-02-25 23_52_19-lv0-67149-42248-2021-2027.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67248_1-6_2019-02-25 23_52_19-lv0-67535-40153-2017-2009.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-12_2019-02-26 00_47_47-lv0-34251-31146-2019-2075.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-12_2019-02-26 00_47_47-lv0-36147-36424-2019-2007.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-12_2019-02-26 00_47_47-lv0-39466-31353-2074-2047.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-13_2019-02-26 00_53_32-lv0-15981-26268-2023-2014.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-13_2019-02-26 00_53_32-lv0-5957-15182-2033-2041.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-14_2019-02-26 00_57_49-lv0-19194-32684-2057-2018.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-3_2019-02-26 00_02_22-lv0-25023-19640-2066-2066.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-3_2019-02-26 00_02_22-lv0-27132-20588-2050-2029.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-3_2019-02-26 00_02_22-lv0-28848-17956-2016-2016.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-3_2019-02-26 00_02_22-lv0-29705-20186-2024-2049.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-4_2019-02-26 00_06_29-lv0-44476-50787-2017-2045.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-4_2019-02-26 00_06_29-lv0-45391-48686-2017-2063.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-29412-29229-2017-2017.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-31516-25647-2025-2012.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-31520-27719-2054-2063.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-33830-25116-2045-2026.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-34403-28461-2072-2081.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-5_2019-02-26 00_11_31-lv0-37057-24798-2012-2072.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-2603-48562-2024-2004.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-2784-50608-2024-2032.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-4908-49211-2073-2105.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-540-48442-2008-2016.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-688-50611-2041-2033.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-7370-50655-2073-2069.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_67251_1-8_2019-02-26 00_25_16-lv0-9472-50739-2080-2046.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-7_2019-02-26 02_22_15-lv0-24557-17188-2000-2104.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-7_2019-02-26 02_22_15-lv0-36192-21210-2012-2053.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-7_2019-02-26 02_22_15-lv0-40728-20696-2075-2064.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-7_2019-02-26 02_22_15-lv0-41219-23459-2047-2035.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-79391-6912-2072-2044.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-81171-9039-2016-2027.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-81540-6990-2055-2005.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-83128-38504-2005-2016.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-83426-40817-2032-2033.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-85836-40477-2011-2022.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-86017-42508-2011-2039.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68000_1-8_2019-02-26 02_28_01-lv0-88083-42481-2049-2082.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68805_1-7_2019-03-14 23_55_06-lv0-42909-22296-2027-2043.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68805_1-7_2019-03-14 23_55_06-lv0-50226-16559-2006-2011.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_68805_1-7_2019-03-14 23_55_06-lv0-54215-56419-2068-2043.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-1_2019-03-14 23_40_58-lv0-47611-63515-2072-2046.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-1_2019-03-14 23_40_58-lv0-63809-55942-2041-2070.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-1_2019-03-14 23_40_58-lv0-64005-53913-2001-2023.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-1_2019-03-14 23_40_58-lv0-65729-50564-2098-2032.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-1_2019-03-14 23_40_58-lv0-66011-52682-2047-2011.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-2_2019-03-14 23_35_35-lv0-38403-61575-2010-2072.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-2_2019-03-14 23_35_35-lv0-46550-17615-2038-2055.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file 2018_69188_1-2_2019-03-14 23_35_35-lv0-49394-18682-2060-2021.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D201803347_2019-05-21 19_31_10-lv0-30701-42947-2018-2044.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D201803347_2019-05-21 19_31_10-lv0-33309-36600-2018-2023.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D201803347_2019-05-21 19_31_10-lv0-34945-46364-2008-2084.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D20180627801_2019-05-21 18_40_34-lv0-12112-37198-2058-2042.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D20180627801_2019-05-21 18_40_34-lv0-5132-31444-2032-2043.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file D20180628401_2019-05-21 18_36_19-lv0-16939-29532-2036-2080.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1900703-2_2019-04-30 09_53_59-lv0-21164-5136-2060-2007.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1900703-2_2019-04-30 09_53_59-lv0-22553-3054-2057-2045.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1900703-2_2019-04-30 09_53_59-lv0-24675-3006-2082-2057.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1900703-2_2019-04-30 09_53_59-lv0-6630-14336-2073-2073.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1901976_2019-04-30 14_03_43-lv0-36154-13777-2013-2044.json\n",
      "sending file to s3\n",
      "file successfully sent\n",
      "writing json file G1901976_2019-04-30 14_03_43-lv0-42750-26583-2019-2008.json\n",
      "sending file to s3\n",
      "file successfully sent\n"
     ]
    }
   ],
   "source": [
    "# Create annotation file for each image\n",
    "for img_num in range(len(bucket['Contents'])):\n",
    "    if img_num%2==1:\n",
    "        file_name = bucket['Contents'][img_num]['Key']\n",
    "        with fs.open('{}/{}'.format(bucketName, file_name)) as f:\n",
    "            data = f.read()\n",
    "            xml_to_json(data, bucketName, SRC_folderName, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket names for training and validation data\n",
    "\n",
    "prefix = 'challenge1-signetringcell-dataset/src_detection_model'\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucketName, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucketName, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucketName, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucketName, validation_annotation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output location where object detection model will be stored\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucketName, 'challenge1-signetringcell-dataset/src_detection_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825641698319.dkr.ecr.us-east-2.amazonaws.com/object-detection:1\n"
     ]
    }
   ],
   "source": [
    "# Get our training uri\n",
    "\n",
    "training_image = sagemaker.image_uris.retrieve(region=sess.boto_region_name, framework='object-detection')\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "# Sagemaker estimator will initiate the training job\n",
    "\n",
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         instance_count=1, \n",
    "                                         instance_type='ml.t3.medium',\n",
    "                                         volume_size = 50,\n",
    "                                         max_run = 360000,\n",
    "                                         input_mode = 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our hyperparameters\n",
    "\n",
    "od_model.set_hyperparameters(base_network='resnet-50',\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=1,\n",
    "                             mini_batch_size=4,\n",
    "                             epochs=30,\n",
    "                             learning_rate=0.001,\n",
    "                             lr_scheduler_step='10',\n",
    "                             lr_scheduler_factor=0.1,\n",
    "                             optimizer='sgd',\n",
    "                             momentum=0.9,\n",
    "                             weight_decay=0.0005,\n",
    "                             overlap_threshold=0.5,\n",
    "                             nms_threshold=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our data channels from the s3 buckets\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data, distribution='FullyReplicated', content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data, distribution='FullyReplicated', content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.inputs.TrainingInput(s3_train_annotation, distribution='FullyReplicated', content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.inputs.TrainingInput(s3_validation_annotation, distribution='FullyReplicated', content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data, \n",
    "                 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'ml.t3.medium' at 'resourceConfig.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge, ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge, ml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge, ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge, ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge, ml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge, ml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p3.8xlarge, ml.m4.4xlarge]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-391d6bdba364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the object detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mod_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateTrainingJob operation: 1 validation error detected: Value 'ml.t3.medium' at 'resourceConfig.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.p4d.24xlarge, ml.c5n.xlarge, ml.p3.16xlarge, ml.m5.large, ml.p2.16xlarge, ml.c4.2xlarge, ml.c5.2xlarge, ml.c4.4xlarge, ml.c5.4xlarge, ml.c5n.18xlarge, ml.g4dn.xlarge, ml.g4dn.12xlarge, ml.c4.8xlarge, ml.g4dn.2xlarge, ml.c5.9xlarge, ml.g4dn.4xlarge, ml.c5.xlarge, ml.g4dn.16xlarge, ml.c4.xlarge, ml.g4dn.8xlarge, ml.c5n.2xlarge, ml.c5n.4xlarge, ml.c5.18xlarge, ml.p3dn.24xlarge, ml.p3.2xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.c5n.9xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p3.8xlarge, ml.m4.4xlarge]"
     ]
    }
   ],
   "source": [
    "# Train the object detection model\n",
    "\n",
    "od_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
